---
layout: post
title: WebMagic初尝试
category: Web
tags: [todo,webspider,javaspider,webmagic,]
keywords: 
description: 第一次编写webmagic爬虫
---


最近看到一个轻量级的java爬虫框架[webmagic](https://github.com/code4craft/webmagic)，文档页面在[webmagic](http://webmagic.io/).<br>

## webmagic组成与设计思想

### 组成

webmagic由四个组件组成(Downloader，PageProcessor，Scheduler，Pipeline)组成。<br>

![总体架构][1]

### 代码分析

看了一下webmagic的源代码，主要就是```HttpClientDownloader```获取html页面，对html页面进行信息抽取，抽取的链接可以继续添加进入爬取队列。<br>

#### 问题：
我目前没看懂```page.addTargetRequests(List<String> list)```，```Page```页面中该方法的说明如下：<br>

```java
public void addTargetRequests(List<String> requests) {
    synchronized (targetRequests) {
        for (String s : requests) {
            if (StringUtils.isBlank(s) || s.equals("#") || s.startsWith("javascript:")) {
                continue;
            }
            s = UrlUtils.canonicalizeUrl(s, url.toString());
            targetRequests.add(new Request(s));
        }
    }
}
```
这段代码将```new Request(url)```添加进入```targetRequests```中，但是目前还未看懂爬虫可以在什么时候发现targetRequests并从中提取信息继续爬的，过几天再来看一看这一部分的源代码


## 写第一个爬虫

### 设置Site

```Site```对象可以设置重试次数，爬取间隔时间，添加```user-agent```，设置字符编码等功能<br>

```java
Site site = Site.me().setCycleRetryTimes(5).setRetryTimes(5).setSleepTime(2000)
    .setUserAgent("Mozilla/5.0 (Windows NT 6.1; WOW64; rv:38.0) Gecko/20100101 Firefox/38.0")
    .addHeader("Accept", "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8")
    .addHeader("Accept-Language", "zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3")
    .setCharset("UTF-8");
```

### 爬取逻辑

爬取逻辑主要写在```Process(Page page)```中<br>

webmagic可以使用正则表达式，CSS选择器，xpath来进行页面抽取：<br>

```xpath
<!--获取页面中类名为"zm-item"的div下h2节点下a节点的链接-->
"//div[@class='zm-item']/h2/a/@href"

<!--获取页面中id为"zh-question-answer-wrap"下div中的内容-->
"//div[@id='zh-question-answer-wrap']/div"
```

```java
public void process(Page page) {
    List<String> questionurls=new ArrayList<>();
    questionurls=page.getHtml().xpath("//div[@class='zm-item']/h2/a/@href").all();
    for(String s:questionurls){
        System.out.println(s);
    }
    page.addTargetRequests(questionurls);
    List<String> answers=page.getHtml().xpath("//div[@id='zh-question-answer-wrap']/div").all();
    
    for(String answer:answers){
        Html html=new Html(answer);
        String author=html.xpath("//div[@class='answer-head']//a[@class='author-link']/text()").toString();
        String vote = html.xpath("//div[@class='zm-votebar']//span[@class='count']/text()").toString();
        String text=html.xpath("//div[@class='zm-editable-content']/text()").toString();
    }
}

```

### 爬虫启动

```java
Spider.create(new Zhihu_XiaoJinMo()).
        addUrl("https://www.zhihu.com/people/xiao-jing-mo/answers").
        addPipeline(new FilePipeline("E:\\webmagic\\")).
        thread(1).
        run();
```

### 持久化存储

webmagic支持存储为本地文本文件，JSON文件，存储进MySQL中。<br>

```java
//存储至E:\\webmagic\\中

Spider.……
    .addPipeline(new FilePipeline("E:\\webmagic\\"))

```

## 实例：知乎爬虫

webmagic目前不支持设定爬取深度，所以该段代码实际上会从萧大的回答页面开始不断爬取。和我最初的目标即只爬取页面还是有些差距。过一阵子改一下。<br>

```java
public class Zhihu_XiaoJinMo implements PageProcessor{
    private final int minumVote=0;
    private Site site = Site.me().setCycleRetryTimes(5).setRetryTimes(5).setSleepTime(2000)
            .setUserAgent("Mozilla/5.0 (Windows NT 6.1; WOW64; rv:38.0) Gecko/20100101 Firefox/38.0")
            .addHeader("Accept", "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8")
            .addHeader("Accept-Language", "zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3")
            .setCharset("UTF-8");

    @Override
    public void process(Page page) {
        List<String> questionurls=new ArrayList<>();
        questionurls=page.getHtml().xpath("//div[@class='zm-item']/h2/a/@href").all();
//        for(String s:questionurls){
//            System.out.println(s);
//        }
//        for(int i=1;i<10;i++){
//            questionurls.add("https://www.zhihu.com/people/xiao-jing-mo/answers?page="+i);
//        }
        //page.addTargetRequests(questionurls);
        //page.addTargetRequest("https://www.zhihu.com/people/xiao-jing-mo/answers");
        page.addTargetRequests(questionurls);
        List<String> answers=page.getHtml().xpath("//div[@id='zh-question-answer-wrap']/div").all();

        boolean exist = false;
        for(String answer:answers){
            Html html=new Html(answer);
            String author=html.xpath("//div[@class='answer-head']//a[@class='author-link']/text()").toString();
            String vote = html.xpath("//div[@class='zm-votebar']//span[@class='count']/text()").toString();
            String text=html.xpath("//div[@class='zm-editable-content']/text()").toString();

            if(Integer.valueOf(vote)>=minumVote) {
                //page.putField("author",html);
                page.putField("author",author);
                page.putField("votes", vote);
                page.putField("text",html.xpath("//div[@class='zm-editable-content']/text()"));
                exist=true;
            }
            if(!exist){
                page.setSkip(true);
            }
        }


    }

    @Override
    public Site getSite() {
        return site;
    }

    public static void main(String[] args){
        String log4jConfPath = "log4j.properties";
        PropertyConfigurator.configure(log4jConfPath);
        Spider.create(new Zhihu_XiaoJinMo()).
                addUrl("https://www.zhihu.com/people/xiao-jing-mo/answers").
                addPipeline(new FilePipeline("E:\\webmagic\\")).
                thread(1).
                run();
    }

}

```


  [1]:   /assets/img/posts/webmagic.jpg